<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills</title>
  <link rel="icon" type="image/x-icon" href="static/images/tum_icon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  
  <style>
    table {
        width: 50%;
        border-collapse: collapse;
        margin: 20px 0;
        font-size: 18px;
        text-align: left;
    }
    th, td {
        padding: 10px;
        border: 1px solid #ddd;
    }
    th {
        background-color: #d3d3d3;
    }
    .gray-row {
        background-color: #f0f0f0;
    }
    caption {
        font-size: 20px;
        font-weight: bold;
        margin-bottom: 10px;
    }
    .footnotes {
        font-size: 14px;
        margin-top: 10px;
    }
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Yuan Meng</a><sup>1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Zhenguo Sun</a><sup>2</sup>,</span>
              <span class="author-block">
                <a target="_blank">Max Fest</a><sup>1</sup>,</span>
              <span class="author-block">
                <a target="_blank">Xukun Li</a><sup>1</sup>,</span>
              <br>
              <span class="author-block">
                <a target="_blank">Zhenshan Bing</a><sup>3,&dagger;</sup>,</span>
              <span class="author-block">
                <a target="_blank">Anois Knoll</a><sup>1</sup>,</span>
                    
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <small><sup>1</sup>School of Computation, Information and Technology, Technical University of Munich, Germany</small>
                      <br><small><sup>2</sup>Beijing Academy of Artificial Intelligence (BAAI), Bejing, China</small>
                      <br><small><sup>3</sup>State Key Laboratory for Novel Software Technology, Nanjing University, China</small>
                      <br><small><sup>&dagger;</sup>Corresponding author: bing@nju.edu.cn</small>
                    </span>
                    
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="./LYRA-v2.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Ghiara/LYRA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                  <!-- Github link -->
                  

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video 1-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4"> -->
        <source src="./static/videos/LYRA-supplementary-video-compressed.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-justified">
        <b>Video demonstrations of LYRA in solving both simulation and real-world long-horizon tasks</b>.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video 1-->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-pyramid">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/task1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-cube">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/task2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-circle">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/task3.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <h2 class="subtitle has-text-justified">
        <b>Video demonstrations of DAHLIA in solving real-world long-horizon tasks</b>. 
        Here we prepared three tasks for the robot agent to complete in the real-world environment.
        The tasks are as follows:
        (1) Pick all fruits and place them on the plate. 
        (2) Stack blocks in a pyramid shape. 
        (3) Make the coffee from the coffee machine.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-pyramid">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/stack_all_zone.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-cube">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/stack_same_size.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-circle">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/stack_same_color.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-ball">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/stack_color_size.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-concentric">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/stack_pos_color.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-odd">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/absolute_pos.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-most">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/aboslute_pos_size.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-divide">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/matching_bowl.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-bisector">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mismatching_bowl.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fixture">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/alternate_color.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-pyramid">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/pyramid.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-cube">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/cube.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-circle">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/circle.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-ball">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/ball.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-concentric">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/concentric.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-odd">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/odd.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-most">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/most.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-divide">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/divide.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-bisector">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/bisector.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fixture">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fixture.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <h2 class="subtitle has-text-justified">
        <b>Simulation demonstrations of various long-horizon tasks from both original LoHoRavens benchmark and generated pool</b>. 
        We test our method in the original LoHoRavens benchmark and the generated pool, where the agent needs to perform up to 20 long-horizon tasks
      </h2>
    </div>
  </div>
</section> -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs)-based code generation for robotic manipulation has recently shown promise by directly translating human instructions into executable code, but existing approaches are limited by language ambiguity, noisy outputs, and limited context windows, which makes long-horizon tasks hard to solve.
            While closed-loop feedback has been explored, approaches that rely solely on LLM guidance frequently fail in extremely long-horizon scenarios due to LLMs' limited reasoning capability in the robotic domain, where such issues are often simple for humans to identify.
            Moreover, corrected knowledge is often stored in improper formats, restricting generalization and causing catastrophic forgetting, which highlights the need for learning reusable and extendable skills.
            To address these issues, we propose a human-in-the-loop lifelong skill learning and code generation framework that encodes feedback into reusable skills and extends their functionality over time.
            An external memory with Retrieval-Augmented Generation and a hint mechanism supports dynamic reuse, enabling robust performance on long-horizon tasks.
            Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world settings, show that our framework achieves a 0.93 success rate (up to 27% higher than baselines) and a 42% efficiency improvement in feedback rounds. 
            It can robustly solve extremely long-horizon tasks such as ``build a house'', which requires planning over 20 primitives.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-widescreen" bis_skin_checked="1">

    <div class="rows" bis_skin_checked="1">


    <!-- Animation. -->
    <div class="rows is-centered " bis_skin_checked="1">
      <div class="row is-full-width" bis_skin_checked="1">
        <h2 class="title is-3"><span class="dcapravens">LYRA</span></h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Framework Overview</h3>
        <div class="content has-text-justified" bis_skin_checked="1">
          <p>
            
          </p>
        </div>
        <img src="./imgs/LYRA-overview.png" class="interpolation-image" alt="Interpolate start reference image.">
        <br>
        <br>
            <b>Fig. 1:</b> Overview of LYRA. 
            
        <br>
        <br>
        <br>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Prompts</h3>
        <div class="content has-text-justified" bis_skin_checked="1">
          <p>
            We use prompt consisting of several functional parts to instruct the LLM to output reasonable content. These parts are imports,
            method explanations, orientations, general requirements, task examples and question. Each part is written in Python style and should 
            guide the LLM to get familiar with the environment where the robot performs task completion, including the way to get information from 
            the environment, as well as the way to control the objects in the environment.

          </p>
        </div>
        <div id="results-carousel" class="carousel results-carousel"> -->
      
          <!-- <div class="item">
           <img src="static/images/import.png" alt="general LRL overview" class="center"/>
           <h2 class="subtitle has-text-justified">
             <b>Imports:</b>. 
             Example of importation that tells the LLM the methods it can directly use, which can also
             include language model programs (LMPs).
           </h2>
         </div>
   
         <div class="item">
           <img src="static/images/explanation.png" alt="Training" class="center"/>
           <h2 class="subtitle has-text-justified">
            <b>Method explanations:</b>
            Example of API introduction, which emphasizes the data format of arguments and return
            values.
           </h2>
         </div>
         <div class="item">
           <img src="static/images/orientation.png" alt="Deployment" class="center"/>
           <h2 class="subtitle has-text-justified">
            <b>Orientations:</b>
            Example of orientation explanation, which describes the relationship between common
            orientation expressions and coordinate axes.
          </h2>
        </div>
        <div class="item">
         <img src="static/images/requirement.png" alt="Success rate" class="center"/>
         <h2 class="subtitle has-text-justified">
           <b>General requirements:</b> 
           Example of overall requirements.
         </h2>
       </div>
       <div class="item">
         <img src="static/images/example.png" alt="t-SNE projection" class="center"/>
         <h2 class="subtitle has-text-justified">
           <b>Task examples:</b> 
           A small set of task examples of increasing difficulty for LMP parse_obj_name(), which should return objects that match the 
           language discription.
         </h2>
       </div>
       <div class="item">
        <img src="static/images/question.png" alt="t-SNE projection" class="center"/>
        <h2 class="subtitle has-text-justified">
          <b>Question:</b> 
          Example of the current task for LMP parse_obj_name().
        </h2>
      </div>
     </div> -->

        <!--/ Re-rendering. -->
      <!-- </div> -->
    </div>

  <!-- </div> -->
</div></section>

<!-- <section class="section">
  <div class="container is-max-widescreen" bis_skin_checked="1">

    <div class="rows" bis_skin_checked="1">

      <div class="rows is-centered " bis_skin_checked="1">
        <div class="row is-full-width" bis_skin_checked="1">
          <h3 class="title is-4">Scalability</h3>
          <div class="content has-text-justified" bis_skin_checked="1">
            <p>
              To assess the scalability and generalization of our framework across different embodiments and task setups, 
              we conduct additional experiments using the CALVIN and Franka Kitchen benchmarks. The average success rates 
              for individual subtasks are presented in Table \ref{tab:scalability}. The results show that our default 
              dual-tunnel DAHLIA setup consistently outperforms its planner-only variation (DAHLIA GPT-4o-mini<sup>P</sup>), 
              demonstrating the effectiveness of its closed-loop feedback mechanism in improving execution reliability. 
              Our framework achieves 100% success in most tasks, including lift block, open drawer, open slide door, kettle, 
              and slider cabinet, while also surpassing the planner-only model in complex tasks requiring precise coordination, 
              such as rotate block (+4%) and turn on switch (+4%) in CALVIN, as well as top burner (+6%) and bottom burner (+8%) 
              in Franka Kitchen. Notably, the planner-only model may struggle in tasks requiring highly accurate task planning and 
              fine-grained manipulation motions. For example, in the "top burner" and "bottom burner" tasks in Franka Kitchen, 
              the robot must accurately reach the correct rotary knob and rotate it counterclockwise by 45 degrees to activate 
              the corresponding burner. Errors in task planning or dexterous execution—such as selecting the wrong switch or rotating 
              in the wrong direction—can lead to complete task failure. In such cases, closed-loop feedback in the dual-tunnel plays a 
              critical role by detecting and correcting execution errors, enabling the planner to refine its actions and complete 
              the global task plan. The performance gap between the two DAHLIA variations is relatively small, as both employ temporal 
              abstraction and CoT-based task planning, ensuring efficient long-horizon primitive execution. However, the closed-loop 
              feedback mechanism can enhance robustness, allowing for error correction and ensuring greater stability in complex environments. 
              These results further validate DAHLIA's robust execution and adaptive reasoning capabilities, demonstrating its ability to 
              generalize across diverse tasks and embodiments.
            </p>

            <table>
              <caption>Average success rates (%) for individual subtasks in CALVIN and Franka Kitchen</caption>
              <tr>
                  <th>CALVIN Tasks</th>
                  <th>&dagger;DAHLIA (GPT-4o-mini<sup>P</sup>)</th>
                  <th>DAHLIA (Ours)</th>
              </tr>
              <tr><td>lift block</td><td>100</td><td>100</td></tr>
              <tr class="gray-row"><td>rotate block</td><td>94</td><td>98</td></tr>
              <tr><td>turn on switch</td><td>96</td><td>100</td></tr>
              <tr class="gray-row"><td>open slide door</td><td>100</td><td>100</td></tr>
              <tr><td>open drawer</td><td>100</td><td>100</td></tr>
              <tr class="gray-row"><td><strong>overall</strong></td><td>98.00</td><td>99.60</td></tr>
              
              <tr>
                  <th>Franka Kitchen Tasks</th>
                  <th>&dagger;DAHLIA (GPT-4o-mini<sup>P</sup>)</th>
                  <th>DAHLIA (Ours)</th>
              </tr>
              <tr><td>microwave</td><td>98</td><td>100</td></tr>
              <tr class="gray-row"><td>kettle</td><td>100</td><td>100</td></tr>
              <tr><td>light</td><td>98</td><td>100</td></tr>
              <tr class="gray-row"><td>top burner</td><td>90</td><td>96</td></tr>
              <tr><td>bottom burner</td><td>90</td><td>98</td></tr>
              <tr class="gray-row"><td>slider cabinet</td><td>100</td><td>100</td></tr>
              <tr><td>hinge cabinet</td><td>96</td><td>98</td></tr>
              <tr class="gray-row"><td><strong>overall</strong></td><td>96.00</td><td>98.86</td></tr>
          </table>
          <div class="footnotes">
              <p><sup>P</sup> indicates the Planner model. &dagger; indicates the framework barely uses a Planner-only tunnel for open-loop control without the Reporter.</p>
          </div>

          </div>
        </div>
      </div>

    </div>
  </div>
</section> -->

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    
    <div class="container">
      
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-odd">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/lift.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-most">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/rotate.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-divide">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/switch.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-bisector">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/drawer.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fixture">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/slide.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fixture">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/kitchen1.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-justified">
        <b>Simulation demonstrations in CALVIN and Franka Kitchen Benchmark</b>. 
        We also test the method in other benchmarks like CALVIN and Franka Kitchen with different robot setup, 
        where the agent needs to perform various tasks in a table-top environment or a kitchen environment. 
        For CALVIN, the agent needs to perform up to five tasks in a row according to language instructions.
        For Franka Kitchen, the agent needs to perform up to 4 sub-tasks in a row according to language instructions.
      </h2>
    </div>
  </div>
</section> -->

<!-- Image carousel -->

<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
            <!-- <iframe src="https://assets-eu.researchsquare.com/files/rs-4353532/v1/d8c2289d1ccbba09ee8c9558.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->

<!-- End video carousel -->






<!-- Paper poster -->

<!--End paper poster -->


<!--BibTex citation -->
  
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
